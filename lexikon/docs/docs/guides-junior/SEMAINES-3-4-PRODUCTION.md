# Semaines 3-4 - Production Hardening
## Guide Condens√© pour D√©veloppeur Junior

**Dur√©e** : 2-3 semaines (60-80h)
**Priorit√©** : üü¢ P2 - RECOMMAND√â
**Objectif** : Renforcer la robustesse pour production

---

## üìã Vue d'Ensemble

| T√¢ches | Dur√©e | Priorit√© |
|--------|-------|----------|
| OAuth GitHub + Google | 8-12h | P2 |
| Monitoring Sentry | 2-3h | P2 |
| Containerisation Docker | 1-2 jours | P2 |
| Tests charge Neo4j | 2-3 jours | P2 |
| M√©triques LLM | 1-2 jours | P2 |

---

## üîê OAuth Implementation (Jours 1-3)

### Objectif
Permettre login avec GitHub et Google

### GitHub OAuth Setup

**1. Cr√©er une OAuth App sur GitHub**
- Aller sur https://github.com/settings/developers
- "New OAuth App"
- Name: `Lexikon Development`
- Homepage URL: `http://localhost:5173`
- Callback URL: `http://localhost:5173/oauth/callback/github`
- Copier Client ID et Client Secret

**2. Configurer backend/.env**
```env
GITHUB_CLIENT_ID=votre_client_id
GITHUB_CLIENT_SECRET=votre_client_secret
```

**3. Impl√©menter dans backend/auth/oauth.py**
```python
from authlib.integrations.starlette_client import OAuth

oauth = OAuth()

oauth.register(
    name='github',
    client_id=os.getenv('GITHUB_CLIENT_ID'),
    client_secret=os.getenv('GITHUB_CLIENT_SECRET'),
    authorize_url='https://github.com/login/oauth/authorize',
    access_token_url='https://github.com/login/oauth/access_token',
    client_kwargs={'scope': 'user:email'},
)

@router.get("/oauth/github")
async def github_login(request: Request):
    redirect_uri = 'http://localhost:5173/oauth/callback/github'
    return await oauth.github.authorize_redirect(request, redirect_uri)

@router.get("/oauth/callback/github")
async def github_callback(request: Request, db: Session = Depends(get_db)):
    token = await oauth.github.authorize_access_token(request)
    user_data = await oauth.github.get('user', token=token)

    # Cr√©er ou r√©cup√©rer l'utilisateur
    email = user_data.get('email')
    existing_user = db.query(User).filter(User.email == email).first()

    if not existing_user:
        new_user = User(
            email=email,
            username=user_data.get('login'),
            full_name=user_data.get('name'),
            hashed_password="oauth-github"  # Pas de mot de passe pour OAuth
        )
        db.add(new_user)
        db.commit()
        db.refresh(new_user)
        user = new_user
    else:
        user = existing_user

    # Cr√©er JWT token
    access_token = create_access_token({"user_id": user.id, "email": user.email})
    refresh_token = create_refresh_token({"user_id": user.id, "email": user.email})

    # Rediriger vers frontend avec token
    return RedirectResponse(
        url=f"http://localhost:5173/oauth/success?token={access_token}"
    )
```

### Google OAuth (similaire)

M√™me approche mais avec :
- Google Cloud Console : https://console.cloud.google.com/
- Cr√©er un projet ‚Üí APIs & Services ‚Üí Credentials ‚Üí OAuth 2.0 Client ID
- Scopes : `openid email profile`

**Test** :
1. Cliquer sur "Login with GitHub" dans le frontend
2. Autoriser l'app GitHub
3. √ätre redirig√© vers le profil avec session active

---

## üìä Monitoring Sentry (Jours 4-5)

### Objectif
Capturer les erreurs en production

### Setup Sentry

**1. Cr√©er un compte sur Sentry.io**
- https://sentry.io/signup/
- Cr√©er un projet "Lexikon Backend" (Python/FastAPI)
- Cr√©er un projet "Lexikon Frontend" (JavaScript/SvelteKit)
- Copier les DSN (Data Source Name)

**2. Backend Integration**
```bash
cd backend
pip install sentry-sdk[fastapi]
pip freeze > requirements.txt
```

**backend/main.py** :
```python
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration

# Initialiser Sentry
sentry_sdk.init(
    dsn=os.getenv("SENTRY_DSN_BACKEND"),
    integrations=[FastApiIntegration()],
    traces_sample_rate=0.1,  # 10% des transactions
    environment=os.getenv("ENVIRONMENT", "development"),
)

# Tester avec un endpoint debug
@app.get("/debug/error")
async def trigger_error():
    1 / 0  # Erreur volontaire pour tester Sentry
```

**3. Frontend Integration**
```bash
npm install @sentry/sveltekit
```

**src/hooks.client.ts** :
```typescript
import * as Sentry from '@sentry/sveltekit';

Sentry.init({
  dsn: import.meta.env.VITE_SENTRY_DSN,
  integrations: [
    new Sentry.BrowserTracing(),
    new Sentry.Replay()
  ],
  tracesSampleRate: 0.1,
  replaysSessionSampleRate: 0.1,
  replaysOnErrorSampleRate: 1.0,
});
```

**Test** :
```bash
# D√©clencher une erreur
curl http://localhost:8000/debug/error

# V√©rifier dans Sentry dashboard
# ‚Üí Erreur doit appara√Ætre avec traceback complet
```

---

## üê≥ Containerisation (Jours 6-8)

### Objectif
Tout l'environnement dans Docker

### Backend Dockerfile

**backend/Dockerfile** :
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# D√©pendances syst√®me
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# D√©pendances Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Code de l'app
COPY . .

# Migrations au d√©marrage
CMD alembic upgrade head && \
    uvicorn main:app --host 0.0.0.0 --port 8000
```

### Frontend Dockerfile

**Dockerfile** (racine) :
```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

FROM node:18-alpine

WORKDIR /app

COPY --from=builder /app/build ./build
COPY --from=builder /app/package.json ./

RUN npm ci --production

CMD ["node", "build"]
```

### Docker Compose Complet

**docker-compose.yml** (d√©commenter les services comment√©s) :
```yaml
version: '3.8'

services:
  postgres:
    # ... (existant)

  neo4j:
    # ... (existant)

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://lexikon:dev-secret@postgres:5432/lexikon
      NEO4J_URI: bolt://neo4j:7687
      JWT_SECRET: ${JWT_SECRET}
    depends_on:
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    volumes:
      - ./backend:/app

  frontend:
    build: .
    ports:
      - "3000:3000"
    environment:
      VITE_API_URL: http://backend:8000
    depends_on:
      - backend
```

**Test** :
```bash
# Build
docker compose build

# D√©marrer
docker compose up

# V√©rifier
curl http://localhost:8000/
curl http://localhost:3000/
```

---

## üß™ Tests de Charge Neo4j (Jours 9-10)

### Objectif
Valider ADR-0001 : Neo4j vs PostgreSQL

### Setup Locust

**Installer Locust**
```bash
pip install locust
```

**Cr√©er tests/load/locustfile.py** :
```python
from locust import HttpUser, task, between

class LexikonUser(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        # Login
        response = self.client.post("/api/auth/login", json={
            "email": "load@test.com",
            "password": "LoadTest123"
        })
        self.token = response.json()["access_token"]

    @task(3)
    def create_term(self):
        self.client.post("/api/terms", json={
            "term": f"Term-{self.random_id()}",
            "definition": "Load test definition",
            "domain": "Testing",
            "level": "beginner"
        }, headers={"Authorization": f"Bearer {self.token}"})

    @task(7)
    def get_terms(self):
        self.client.get("/api/terms", headers={"Authorization": f"Bearer {self.token}"})

    def random_id(self):
        import random
        return random.randint(1, 100000)
```

**Ex√©cuter** :
```bash
# D√©marrer backend
cd backend && uvicorn main:app

# Dans un autre terminal
locust -f tests/load/locustfile.py

# Ouvrir http://localhost:8089
# - Number of users: 100
# - Spawn rate: 10/s
# - Host: http://localhost:8000

# Lancer et observer :
# - Requ√™tes/sec
# - Temps de r√©ponse moyen
# - Erreurs
```

### Benchmarking Neo4j vs PostgreSQL

**Sc√©nario de test** :
1. G√©n√©rer 5000 termes avec relations
2. Requ√™te : "Trouver tous les termes li√©s √† 'Intelligence Artificielle' sur 3 niveaux"
3. Mesurer temps de r√©ponse

**PostgreSQL (WITH RECURSIVE)** :
```sql
WITH RECURSIVE term_tree AS (
  SELECT id, term, 1 as depth
  FROM terms
  WHERE term = 'Intelligence Artificielle'

  UNION ALL

  SELECT t.id, t.term, tt.depth + 1
  FROM terms t
  JOIN relationships r ON t.id = r.target_id
  JOIN term_tree tt ON r.source_id = tt.id
  WHERE tt.depth < 3
)
SELECT * FROM term_tree;
```

**Neo4j (MATCH ... DEPTH)** :
```cypher
MATCH path = (start:Term {name: "Intelligence Artificielle"})-[:RELATED_TO*1..3]->(related)
RETURN related.name, length(path)
```

**D√©cision** :
- Si Neo4j <50ms et PostgreSQL >200ms ‚Üí Garder Neo4j
- Si Neo4j ~100ms et PostgreSQL ~120ms ‚Üí Migrer vers PostgreSQL (simplifier stack)

---

## üìà M√©triques LLM (Jours 11-12)

### Objectif
Mesurer l'impact sur la qualit√© LLM (-30% erreurs s√©mantiques)

### Setup Prometheus + Grafana

**docker-compose.yml** (ajouter) :
```yaml
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
```

### Instrumenter le Code

**backend/main.py** :
```python
from prometheus_client import Counter, Histogram, generate_latest

# M√©triques
llm_requests = Counter('llm_requests_total', 'Total LLM requests', ['model', 'endpoint'])
llm_errors = Counter('llm_semantic_errors', 'Semantic errors detected', ['type'])
llm_latency = Histogram('llm_response_time', 'LLM response time')

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")

# Utiliser dans les endpoints
@llm_latency.time()
async def call_llm(prompt: str):
    llm_requests.labels(model="gpt-4", endpoint="validate_term").inc()
    # ... appel LLM
    if semantic_error_detected:
        llm_errors.labels(type="ambiguous_definition").inc()
```

### Dashboard Grafana

1. Ouvrir http://localhost:3001
2. Login: admin / admin
3. Add data source ‚Üí Prometheus (http://prometheus:9090)
4. Create dashboard :
   - Graphe : `rate(llm_requests_total[5m])`
   - Graphe : `rate(llm_semantic_errors[1h])`
   - Calcul : `(llm_semantic_errors / llm_requests_total) * 100` ‚Üí % erreurs

---

## ‚úÖ Checklist Finale Semaines 3-4

- [ ] OAuth GitHub fonctionnel
- [ ] OAuth Google fonctionnel
- [ ] Sentry capture les erreurs
- [ ] Docker Compose d√©marre toute la stack
- [ ] Benchmarks Neo4j vs PostgreSQL document√©s
- [ ] M√©triques LLM collect√©es dans Grafana

**Test final** :
```bash
# 1. Docker Compose complet
docker compose up

# 2. Login avec GitHub
# Ouvrir http://localhost:3000/login ‚Üí Cliquer GitHub ‚Üí Succ√®s

# 3. D√©clencher erreur Sentry
curl http://localhost:8000/debug/error
# ‚Üí V√©rifier dans Sentry dashboard

# 4. V√©rifier m√©triques
curl http://localhost:8000/metrics
# ‚Üí Devrait afficher m√©triques Prometheus
```

üéâ **Production Ready !** ‚úÖ

---

**Annexes** : [Debugging Guide](./ANNEXE-B-DEBUGGING.md)
