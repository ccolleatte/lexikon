# üìä Analyse Critique Approfondie - Projet Lexicon & Approche Gemini (v0.3)
## Partie 2 : Recommandations Constructives et Architecture

**Analyste** : Claude Opus
**Date** : 2025-11-11
**Focus** : Solutions concr√®tes et roadmap d'impl√©mentation

---

## 4. Recommandations Constructives pour v0.3

### 4.1 Architecture Technique Compl√®te

#### üèóÔ∏è Stack recommand√©e et justifi√©e

```yaml
# Architecture v0.3 - Microservices modulaires

api_gateway:
  technology: Kong / Traefik
  purpose: Rate limiting, auth, routing
  justification: Scalabilit√©, s√©curit√© centralis√©e

core_api:
  technology: FastAPI (Python 3.11+)
  justification:
    - Async native (performance)
    - OpenAPI auto-g√©n√©r√©e
    - Pydantic validation
    - GraphQL compatible (Strawberry)

graph_database:
  primary: Neo4j Community 5.x
  fallback: PostgreSQL 15 + Apache AGE
  decision_criteria:
    - POC benchmark sur 100k termes
    - Requ√™tes graphe depth=5
    - Si latence Neo4j < 50% PostgreSQL ‚Üí Neo4j

relational_database:
  technology: PostgreSQL 15
  extensions:
    - pgvector (embeddings)
    - pg_trgm (fuzzy search)
    - temporal_tables (versioning)
  purpose: M√©tadonn√©es, users, validation

vector_store:
  technology: pgvector
  alternative: Weaviate (si > 1M embeddings)
  embedding_model: sentence-transformers/all-mpnet-base-v2
  dimensions: 768
  justification:
    - Local, pas de co√ªt API
    - Multilingue
    - Performance suffisante

cache:
  technology: Redis 7 + RedisJSON
  patterns:
    - Session validation (TTL 1h)
    - Termes fr√©quents (TTL 24h)
    - API responses (TTL 5min)

task_queue:
  technology: Celery + Redis
  workers:
    - extraction_worker (corpus processing)
    - embedding_worker (vectorization)
    - export_worker (RDF/JSON generation)

search_engine:
  technology: Elasticsearch 8 (optional)
  purpose: Full-text search avanc√©
  activation: Si > 10k termes

frontend:
  framework: SvelteKit 2.0
  ui_components: Tailwind UI
  state: Svelte stores + TanStack Query
  graph_viz: D3.js v7
  justification:
    - Performance (SSR + hydratation)
    - DX excellent
    - Bundle size minimal

monitoring:
  metrics: Prometheus + Grafana
  logs: Loki + Promtail
  errors: Sentry
  uptime: Better Stack
```

#### üèóÔ∏è Architecture de d√©ploiement

```yaml
# Infrastructure as Code (Terraform)

development:
  orchestration: Docker Compose
  services:
    - neo4j:5.15
    - postgres:15-pgvector
    - redis:7-alpine
    - api:latest
    - frontend:latest

staging:
  platform: Kubernetes (k3s)
  nodes: 2 (4 vCPU, 16GB RAM each)
  storage: 100GB SSD
  backup: Daily snapshots

production:
  platform: Kubernetes (EKS/GKE)
  nodes: 3-5 (autoscaling)
  database: Managed (RDS, Aura Neo4j)
  cdn: CloudFlare
  monitoring: DataDog / New Relic
```

### 4.2 Mod√®le de Donn√©es Complet

#### üìä Sch√©ma relationnel PostgreSQL

```sql
-- ============================================
-- CORE ENTITIES
-- ============================================

-- Hi√©rarchie des domaines (r√©cursive)
CREATE TABLE domains (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) UNIQUE NOT NULL,
    slug VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    parent_id UUID REFERENCES domains(id),
    icon VARCHAR(50),
    color VARCHAR(7), -- Hex color
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_parent (parent_id),
    INDEX idx_slug (slug)
);

-- Types de relations (m√©ta-mod√®le)
CREATE TABLE relation_types (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(50) UNIQUE NOT NULL,
    label_forward VARCHAR(100), -- "is a"
    label_reverse VARCHAR(100), -- "has subclass"
    description TEXT,
    is_symmetric BOOLEAN DEFAULT FALSE,
    is_transitive BOOLEAN DEFAULT FALSE,
    is_reflexive BOOLEAN DEFAULT FALSE,
    domain_constraints JSONB, -- {"source": ["Concept"], "target": ["Concept"]}
    cardinality VARCHAR(10), -- '1:1', '1:N', 'N:N'
    inference_rules JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Termes avec versioning natif
CREATE TABLE terms (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    label VARCHAR(200) NOT NULL,
    slug VARCHAR(200) NOT NULL,
    domain_id UUID REFERENCES domains(id) NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'draft',
    status_changed_at TIMESTAMP DEFAULT NOW(),
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    deleted_at TIMESTAMP, -- Soft delete
    CONSTRAINT status_check CHECK (status IN ('draft', 'proposed', 'in_review', 'validated', 'deprecated')),
    INDEX idx_domain_status (domain_id, status),
    INDEX idx_label_trgm USING gin (label gin_trgm_ops), -- Fuzzy search
    UNIQUE (domain_id, slug)
);

-- D√©finitions versionn√©es (temporal table)
CREATE TABLE definitions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    version INT NOT NULL DEFAULT 1,
    short_definition TEXT NOT NULL,
    long_definition TEXT NOT NULL,
    context_specific TEXT,
    usage_local BOOLEAN DEFAULT FALSE,
    usage_notes TEXT,
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    valid_from TIMESTAMP DEFAULT NOW(),
    valid_to TIMESTAMP DEFAULT '9999-12-31',
    UNIQUE (term_id, version),
    INDEX idx_term_version (term_id, version DESC)
);

-- Relations ontologiques avec m√©tadonn√©es riches
CREATE TABLE ontological_relations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    relation_type_id UUID REFERENCES relation_types(id),
    source_term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    target_term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    strength FLOAT DEFAULT 1.0 CHECK (strength BETWEEN 0 AND 1),
    confidence FLOAT DEFAULT 1.0 CHECK (confidence BETWEEN 0 AND 1),
    evidence TEXT,
    temporal_qualifier JSONB, -- {"from": "1800", "to": "1900"}
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    validated_at TIMESTAMP,
    validated_by UUID REFERENCES users(id),
    CHECK (source_term_id != target_term_id),
    INDEX idx_source_type (source_term_id, relation_type_id),
    INDEX idx_target_type (target_term_id, relation_type_id),
    UNIQUE (relation_type_id, source_term_id, target_term_id)
);

-- ============================================
-- METADATA & CONTEXT
-- ============================================

-- Auteurs et √©coles de pens√©e
CREATE TABLE authors (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(200) NOT NULL,
    birth_year INT,
    death_year INT,
    nationality VARCHAR(100),
    school_of_thought VARCHAR(200),
    biography TEXT,
    wikipedia_url VARCHAR(500),
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_name (name)
);

-- Association termes-auteurs avec r√¥le
CREATE TABLE term_authors (
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    author_id UUID REFERENCES authors(id) ON DELETE CASCADE,
    role VARCHAR(50) NOT NULL DEFAULT 'contributor',
    contribution_note TEXT,
    PRIMARY KEY (term_id, author_id, role),
    CONSTRAINT role_check CHECK (role IN ('creator', 'contributor', 'critic', 'popularizer'))
);

-- Citations et sources
CREATE TABLE citations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    quote TEXT NOT NULL,
    source_title VARCHAR(500),
    source_author VARCHAR(200),
    source_year INT,
    source_publisher VARCHAR(200),
    source_page VARCHAR(50),
    source_url VARCHAR(500),
    source_doi VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_term (term_id)
);

-- Synonymes et variantes
CREATE TABLE synonyms (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    synonym VARCHAR(200) NOT NULL,
    language VARCHAR(10) DEFAULT 'fr',
    type VARCHAR(50) NOT NULL DEFAULT 'synonym',
    region VARCHAR(100), -- "Qu√©bec", "France", etc.
    historical_period VARCHAR(100),
    note TEXT,
    CONSTRAINT type_check CHECK (type IN ('synonym', 'variant', 'abbreviation', 'antonym', 'translation')),
    INDEX idx_term (term_id),
    INDEX idx_synonym (synonym)
);

-- ============================================
-- VALIDATION & QUALITY
-- ============================================

-- Workflow de validation HITL
CREATE TABLE validations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    definition_id UUID REFERENCES definitions(id),
    validator_id UUID REFERENCES users(id),
    validation_type VARCHAR(50) NOT NULL,
    decision VARCHAR(20) NOT NULL,
    checklist_results JSONB,
    comments TEXT,
    quality_score FLOAT,
    created_at TIMESTAMP DEFAULT NOW(),
    CONSTRAINT type_check CHECK (validation_type IN ('initial', 'revision', 'periodic', 'disputed')),
    CONSTRAINT decision_check CHECK (decision IN ('approved', 'rejected', 'needs_work')),
    INDEX idx_term_validator (term_id, validator_id)
);

-- Conflits de validation
CREATE TABLE validation_conflicts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    conflict_type VARCHAR(50) NOT NULL,
    validator1_id UUID REFERENCES users(id),
    validator1_decision VARCHAR(20),
    validator2_id UUID REFERENCES users(id),
    validator2_decision VARCHAR(20),
    arbitrator_id UUID REFERENCES users(id),
    resolution VARCHAR(20),
    resolution_notes TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    resolved_at TIMESTAMP,
    INDEX idx_term_pending (term_id, resolved_at)
);

-- Questions de comp√©tence
CREATE TABLE competency_questions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    question TEXT NOT NULL,
    expected_answer TEXT,
    difficulty INT CHECK (difficulty BETWEEN 1 AND 5),
    evaluation_criteria TEXT,
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_term_difficulty (term_id, difficulty)
);

-- ============================================
-- AI & EMBEDDINGS
-- ============================================

-- Embeddings pour recherche s√©mantique
CREATE TABLE term_embeddings (
    term_id UUID PRIMARY KEY REFERENCES terms(id) ON DELETE CASCADE,
    embedding vector(768), -- all-mpnet-base-v2
    embedding_text TEXT, -- Texte utilis√© pour g√©n√©ration
    model VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index pour recherche vectorielle
CREATE INDEX idx_embedding_ivfflat ON term_embeddings
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Cache des requ√™tes LLM
CREATE TABLE llm_cache (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    query_hash VARCHAR(64) UNIQUE NOT NULL, -- SHA256 du prompt
    query_text TEXT,
    enriched_prompt JSONB,
    terms_detected UUID[], -- Array of term IDs
    model VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    accessed_at TIMESTAMP DEFAULT NOW(),
    access_count INT DEFAULT 1,
    INDEX idx_hash (query_hash),
    INDEX idx_accessed (accessed_at)
);

-- ============================================
-- ANALYTICS & METRICS
-- ============================================

-- M√©triques d'usage des termes
CREATE TABLE term_usage_metrics (
    term_id UUID REFERENCES terms(id) ON DELETE CASCADE,
    date DATE NOT NULL,
    api_requests INT DEFAULT 0,
    llm_injections INT DEFAULT 0,
    search_appearances INT DEFAULT 0,
    validation_views INT DEFAULT 0,
    PRIMARY KEY (term_id, date),
    INDEX idx_date (date)
);

-- Audit trail complet
CREATE TABLE audit_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    action VARCHAR(50) NOT NULL,
    entity_type VARCHAR(50) NOT NULL,
    entity_id UUID NOT NULL,
    old_values JSONB,
    new_values JSONB,
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_user_date (user_id, created_at DESC),
    INDEX idx_entity (entity_type, entity_id)
);
```

#### üìä Mod√®le Neo4j pour le graphe ontologique

```cypher
// ============================================
// NEO4J GRAPH MODEL
// ============================================

// Contraintes d'unicit√©
CREATE CONSTRAINT term_unique IF NOT EXISTS
FOR (t:Term) REQUIRE t.id IS UNIQUE;

CREATE CONSTRAINT domain_unique IF NOT EXISTS
FOR (d:Domain) REQUIRE d.id IS UNIQUE;

CREATE CONSTRAINT author_unique IF NOT EXISTS
FOR (a:Author) REQUIRE a.id IS UNIQUE;

// Index pour performance
CREATE INDEX term_label IF NOT EXISTS
FOR (t:Term) ON (t.label);

CREATE INDEX term_domain IF NOT EXISTS
FOR (t:Term) ON (t.domain_id);

// Mod√®le de n≈ìuds
(:Term {
    id: "uuid",
    label: "string",
    short_def: "string",
    long_def: "string",
    domain_id: "uuid",
    status: "string",
    embedding: [float], // 768 dimensions
    created_at: datetime,
    popularity_score: float
})

(:Domain {
    id: "uuid",
    name: "string",
    slug: "string",
    parent_id: "uuid"
})

(:Author {
    id: "uuid",
    name: "string",
    school: "string"
})

(:Citation {
    id: "uuid",
    quote: "string",
    source: "string"
})

// Relations typ√©es
(:Term)-[:IS_A {strength: 0.9, confidence: 0.95}]->(:Term)
(:Term)-[:PART_OF {strength: 0.8}]->(:Term)
(:Term)-[:INFLUENCED_BY {period: "19th century"}]->(:Author)
(:Term)-[:EMPLOYS]->(:Term)
(:Term)-[:OPPOSES]->(:Term)
(:Term)-[:IN_DOMAIN]->(:Domain)
(:Term)-[:HAS_CITATION]->(:Citation)
(:Author)-[:BELONGS_TO]->(:School)

// Requ√™tes optimis√©es

// 1. Trouver tous les concepts li√©s (profondeur variable)
MATCH path = (start:Term {label: $term})-[*1..3]-(related:Term)
WHERE ALL(r IN relationships(path) WHERE r.strength > 0.5)
RETURN path, related
ORDER BY length(path), related.popularity_score DESC
LIMIT 50;

// 2. D√©tection de cycles (validation ontologique)
MATCH path = (t:Term)-[r:IS_A*]->(t)
RETURN t.label as cycle_term, length(path) as cycle_length;

// 3. Inf√©rence transitive
MATCH (a:Term)-[:IS_A]->(b:Term)-[:IS_A]->(c:Term)
WHERE NOT EXISTS((a)-[:IS_A]->(c))
MERGE (a)-[:IS_A {inferred: true, confidence: 0.8}]->(c);

// 4. Recherche s√©mantique par embeddings (avec plugin vector)
CALL db.index.vector.queryNodes('term-embeddings', 10, $query_embedding)
YIELD node, score
WHERE score > 0.7
RETURN node.label, node.short_def, score
ORDER BY score DESC;
```

### 4.3 Workflow HITL Optimis√©

#### üîÑ Pipeline de validation intelligent

```python
# Pseudo-code du workflow de validation

class ValidationPipeline:
    def __init__(self):
        self.priority_queue = PriorityQueue()
        self.validators_pool = ValidatorPool()

    def calculate_priority_score(self, term):
        """Score de priorisation multi-crit√®res"""
        return (
            term.corpus_frequency * 0.3 +
            term.user_requests * 0.25 +
            term.graph_centrality * 0.2 +
            term.semantic_ambiguity * 0.15 +
            term.domain_importance * 0.1
        )

    def assign_validators(self, term):
        """Attribution intelligente des validateurs"""
        # Trouve experts du domaine
        domain_experts = self.validators_pool.get_by_domain(term.domain)

        # √âvite conflits d'int√©r√™ts
        available = [e for e in domain_experts
                    if e.id not in term.authors]

        # S√©lectionne par charge de travail
        return sorted(available, key=lambda e: e.current_load)[:2]

    def auto_enrich(self, term):
        """Pr√©-remplissage intelligent"""
        enrichments = {}

        # 1. Extraction Wikidata
        if wikidata_match := self.search_wikidata(term.label):
            enrichments['definition'] = wikidata_match.description
            enrichments['aliases'] = wikidata_match.aliases

        # 2. Extraction citations (Semantic Scholar API)
        papers = self.search_papers(term.label, limit=5)
        enrichments['citations'] = [p.abstract for p in papers]

        # 3. Relations candidates (embeddings)
        similar_terms = self.find_similar_terms(term.embedding, k=10)
        enrichments['suggested_relations'] = [
            {'type': 'related_to', 'target': t.id, 'score': score}
            for t, score in similar_terms
        ]

        # 4. Auteurs d√©tect√©s (NER sur corpus)
        enrichments['detected_authors'] = self.extract_authors(term.context)

        return enrichments

    def validation_checklist(self, term_data):
        """Checklist automatis√©e avec scoring"""
        checks = {
            'has_short_def': len(term_data.short_def) > 10,
            'has_long_def': len(term_data.long_def) > 50,
            'has_source': len(term_data.citations) > 0,
            'has_relations': len(term_data.relations) > 0,
            'no_circular_ref': not self.has_circular_relations(term_data),
            'unique_in_domain': not self.is_duplicate(term_data),
            'follows_naming': self.validate_naming_convention(term_data.label),
            'quality_score': self.calculate_quality_score(term_data)
        }

        return {
            'passed': all(checks.values()),
            'details': checks,
            'score': sum(checks.values()) / len(checks) * 100
        }

    def handle_conflict(self, term_id, validator1, validator2):
        """Gestion des d√©saccords"""
        conflict = ValidationConflict(
            term_id=term_id,
            validator1=validator1,
            validator2=validator2
        )

        # Strat√©gie 1 : Troisi√®me expert
        if arbitrator := self.find_senior_expert(term.domain):
            conflict.assign_arbitrator(arbitrator)

        # Strat√©gie 2 : Vote communautaire
        elif self.community_size(term.domain) > 10:
            conflict.open_community_vote(duration_days=3)

        # Strat√©gie 3 : Escalade au comit√©
        else:
            conflict.escalate_to_committee()

        return conflict
```

#### üéØ M√©triques de performance HITL

```sql
-- Dashboard m√©triques validation

-- Efficacit√© validateurs
CREATE VIEW validator_performance AS
SELECT
    u.name as validator,
    COUNT(DISTINCT v.term_id) as terms_validated,
    AVG(v.quality_score) as avg_quality,
    AVG(EXTRACT(EPOCH FROM (v.created_at - t.created_at))/3600) as avg_hours_to_validate,
    SUM(CASE WHEN vc.id IS NOT NULL THEN 1 ELSE 0 END) as conflicts,
    COUNT(DISTINCT v.term_id) * 100.0 / NULLIF(COUNT(DISTINCT t.id), 0) as coverage_percent
FROM users u
JOIN validations v ON u.id = v.validator_id
JOIN terms t ON v.term_id = t.id
LEFT JOIN validation_conflicts vc ON v.term_id = vc.term_id
WHERE v.created_at > NOW() - INTERVAL '30 days'
GROUP BY u.id, u.name;

-- Pipeline sant√©
CREATE VIEW pipeline_health AS
SELECT
    COUNT(CASE WHEN status = 'proposed' THEN 1 END) as pending_validation,
    COUNT(CASE WHEN status = 'in_review' THEN 1 END) as in_review,
    COUNT(CASE WHEN status = 'validated' THEN 1 END) as validated,
    AVG(CASE
        WHEN status = 'validated'
        THEN EXTRACT(EPOCH FROM (status_changed_at - created_at))/86400
    END) as avg_days_to_validation,
    PERCENTILE_CONT(0.5) WITHIN GROUP (
        ORDER BY EXTRACT(EPOCH FROM (status_changed_at - created_at))/86400
    ) as median_days_to_validation
FROM terms
WHERE created_at > NOW() - INTERVAL '30 days';

-- Qualit√© ontologique
CREATE VIEW ontology_quality AS
SELECT
    d.name as domain,
    COUNT(DISTINCT t.id) as total_terms,
    COUNT(DISTINCT r.id) as total_relations,
    AVG(cardinality) as avg_relations_per_term,
    COUNT(DISTINCT CASE WHEN t.status = 'validated' THEN t.id END) * 100.0 /
        NULLIF(COUNT(DISTINCT t.id), 0) as validation_rate,
    COUNT(DISTINCT c.id) * 100.0 / NULLIF(COUNT(DISTINCT t.id), 0) as citation_coverage
FROM domains d
JOIN terms t ON d.id = t.domain_id
LEFT JOIN ontological_relations r ON t.id IN (r.source_term_id, r.target_term_id)
LEFT JOIN citations c ON t.id = c.term_id
GROUP BY d.id, d.name;
```

### 4.4 Int√©gration LLM Concr√®te

#### ü§ñ Architecture d'enrichissement contextuel

```python
from typing import List, Dict, Optional
import hashlib
from dataclasses import dataclass

@dataclass
class OntologyContext:
    """Contexte ontologique pour enrichissement LLM"""
    query: str
    detected_terms: List[Dict]
    relations_graph: Dict
    definitions: Dict
    citations: List[str]

class LLMIntegrationService:
    def __init__(self, ontology_service, embedding_service, cache):
        self.ontology = ontology_service
        self.embeddings = embedding_service
        self.cache = cache

    async def enrich_prompt(
        self,
        user_prompt: str,
        domain: str = None,
        mode: str = 'balanced',
        max_context_tokens: int = 2000
    ) -> Dict:
        """
        Enrichit un prompt utilisateur avec le contexte ontologique

        Modes:
        - 'minimal': D√©finitions courtes uniquement
        - 'balanced': D√©finitions + relations directes
        - 'comprehensive': Graphe complet + citations
        """

        # 1. Check cache
        cache_key = self._compute_cache_key(user_prompt, domain, mode)
        if cached := await self.cache.get(cache_key):
            return cached

        # 2. D√©tection des termes
        detected = await self._detect_terms(user_prompt, domain)

        # 3. Construction du graphe de contexte
        context_graph = await self._build_context_graph(
            detected,
            depth=2 if mode == 'comprehensive' else 1
        )

        # 4. Optimisation du contexte (token budget)
        optimized_context = self._optimize_context(
            context_graph,
            max_context_tokens
        )

        # 5. Formatage pour injection
        enriched = self._format_for_llm(optimized_context, mode)

        # 6. Cache result
        await self.cache.set(cache_key, enriched, ttl=3600)

        return enriched

    async def _detect_terms(self, text: str, domain: str = None) -> List[Dict]:
        """D√©tection multi-strat√©gies des termes ontologiques"""
        detected = []

        # Strat√©gie 1: Exact match (case insensitive)
        terms = await self.ontology.get_all_terms(domain)
        for term in terms:
            if term.label.lower() in text.lower():
                detected.append({
                    'term_id': term.id,
                    'label': term.label,
                    'confidence': 1.0,
                    'method': 'exact_match'
                })

        # Strat√©gie 2: Fuzzy match (Levenshtein)
        tokens = self._tokenize(text)
        for token in tokens:
            if matches := await self.ontology.fuzzy_search(token, threshold=0.85):
                for match in matches[:3]:
                    detected.append({
                        'term_id': match.id,
                        'label': match.label,
                        'confidence': match.score,
                        'method': 'fuzzy_match'
                    })

        # Strat√©gie 3: Embedding similarity
        text_embedding = await self.embeddings.encode(text)
        similar = await self.ontology.search_by_embedding(
            text_embedding,
            k=5,
            threshold=0.7
        )
        for term, score in similar:
            detected.append({
                'term_id': term.id,
                'label': term.label,
                'confidence': score,
                'method': 'semantic_match'
            })

        # D√©duplicate and sort by confidence
        seen = set()
        unique = []
        for item in sorted(detected, key=lambda x: x['confidence'], reverse=True):
            if item['term_id'] not in seen:
                seen.add(item['term_id'])
                unique.append(item)

        return unique[:10]  # Top 10 terms max

    async def _build_context_graph(
        self,
        detected_terms: List[Dict],
        depth: int = 1
    ) -> Dict:
        """Construction du graphe de contexte autour des termes d√©tect√©s"""

        graph = {
            'nodes': {},
            'edges': []
        }

        # Ajoute les termes d√©tect√©s
        for term_info in detected_terms:
            term = await self.ontology.get_term(term_info['term_id'])
            graph['nodes'][term.id] = {
                'label': term.label,
                'definition_short': term.definition_short,
                'definition_long': term.definition_long,
                'domain': term.domain,
                'detection_confidence': term_info['confidence']
            }

            # R√©cup√®re les relations (BFS limit√©)
            if depth > 0:
                relations = await self.ontology.get_relations(
                    term.id,
                    max_depth=depth
                )
                for rel in relations:
                    graph['edges'].append({
                        'source': rel.source_id,
                        'target': rel.target_id,
                        'type': rel.type,
                        'strength': rel.strength
                    })

                    # Ajoute les termes li√©s
                    if rel.target_id not in graph['nodes']:
                        target = await self.ontology.get_term(rel.target_id)
                        graph['nodes'][rel.target_id] = {
                            'label': target.label,
                            'definition_short': target.definition_short,
                            'domain': target.domain
                        }

        return graph

    def _optimize_context(
        self,
        context_graph: Dict,
        max_tokens: int
    ) -> Dict:
        """Optimise le contexte pour respecter le budget de tokens"""

        # Estimation tokens (rough: 1 token ‚âà 4 chars)
        def estimate_tokens(text):
            return len(text) / 4

        optimized = {'nodes': {}, 'edges': []}
        current_tokens = 0

        # Priorise les n≈ìuds par confidence
        sorted_nodes = sorted(
            context_graph['nodes'].items(),
            key=lambda x: x[1].get('detection_confidence', 0),
            reverse=True
        )

        for node_id, node_data in sorted_nodes:
            node_tokens = estimate_tokens(
                node_data['label'] +
                node_data.get('definition_short', '') +
                node_data.get('definition_long', '')
            )

            if current_tokens + node_tokens <= max_tokens:
                # Tronque la d√©finition longue si n√©cessaire
                if current_tokens + node_tokens > max_tokens * 0.9:
                    node_data['definition_long'] = \
                        node_data.get('definition_long', '')[:200] + '...'

                optimized['nodes'][node_id] = node_data
                current_tokens += node_tokens
            else:
                break

        # Ajoute les edges pertinentes
        for edge in context_graph['edges']:
            if (edge['source'] in optimized['nodes'] and
                edge['target'] in optimized['nodes']):
                optimized['edges'].append(edge)

        return optimized

    def _format_for_llm(self, context: Dict, mode: str) -> Dict:
        """Formate le contexte pour injection dans le prompt LLM"""

        if mode == 'minimal':
            # Format compact
            definitions = []
            for node in context['nodes'].values():
                definitions.append(f"‚Ä¢ {node['label']}: {node['definition_short']}")

            system_context = (
                "Contexte ontologique:\n" + "\n".join(definitions)
            )

        elif mode == 'balanced':
            # Format structur√©
            terms_section = []
            for node_id, node in context['nodes'].items():
                terms_section.append(
                    f"**{node['label']}** [{node['domain']}]\n"
                    f"D√©finition: {node['definition_short']}\n"
                )

            relations_section = []
            for edge in context['edges']:
                source = context['nodes'][edge['source']]['label']
                target = context['nodes'][edge['target']]['label']
                relations_section.append(
                    f"‚Ä¢ {source} {edge['type']} {target}"
                )

            system_context = (
                "## Contexte Ontologique\n\n"
                "### Termes identifi√©s:\n" +
                "\n".join(terms_section) + "\n\n" +
                "### Relations:\n" +
                "\n".join(relations_section)
            )

        else:  # comprehensive
            # Format JSON-LD complet
            jsonld = {
                "@context": "https://lexicon.ai/ontology/v1",
                "@graph": [
                    {
                        "@id": f"term:{node_id}",
                        "label": node['label'],
                        "definition": {
                            "short": node['definition_short'],
                            "long": node.get('definition_long', '')
                        },
                        "domain": node['domain']
                    }
                    for node_id, node in context['nodes'].items()
                ],
                "relations": [
                    {
                        "source": f"term:{edge['source']}",
                        "predicate": edge['type'],
                        "target": f"term:{edge['target']}",
                        "strength": edge['strength']
                    }
                    for edge in context['edges']
                ]
            }

            system_context = f"Contexte ontologique structur√©:\n```json\n{jsonld}\n```"

        return {
            'system_context': system_context,
            'detected_terms': list(context['nodes'].keys()),
            'injection_mode': mode,
            'token_count': len(system_context) // 4
        }

    def _compute_cache_key(self, prompt: str, domain: str, mode: str) -> str:
        """G√©n√®re une cl√© de cache d√©terministe"""
        content = f"{prompt}:{domain}:{mode}"
        return hashlib.sha256(content.encode()).hexdigest()
```

#### ü§ñ Benchmark de r√©duction d'erreurs

```python
# Protocole de test pour mesurer -30% erreurs

class OntologyBenchmark:
    def __init__(self):
        self.test_dataset = self.load_ambiguous_prompts()
        self.evaluators = self.recruit_experts(n=3)

    def run_benchmark(self):
        results = {
            'baseline': [],
            'with_ontology': []
        }

        for prompt in self.test_dataset:
            # 1. Baseline (sans ontologie)
            baseline_response = self.generate_llm_response(
                prompt,
                use_ontology=False
            )

            # 2. Avec ontologie
            enriched_prompt = self.enrich_with_ontology(prompt)
            ontology_response = self.generate_llm_response(
                enriched_prompt,
                use_ontology=True
            )

            # 3. √âvaluation humaine
            baseline_scores = self.evaluate_response(
                prompt,
                baseline_response
            )
            ontology_scores = self.evaluate_response(
                prompt,
                ontology_response
            )

            results['baseline'].append(baseline_scores)
            results['with_ontology'].append(ontology_scores)

        # 4. Calcul m√©triques
        metrics = self.compute_metrics(results)
        return metrics

    def evaluate_response(self, prompt, response):
        """√âvaluation par experts avec crit√®res stricts"""
        scores = []

        for evaluator in self.evaluators:
            score = evaluator.evaluate({
                'prompt': prompt,
                'response': response,
                'criteria': {
                    'semantic_accuracy': (0, 5),  # Pr√©cision s√©mantique
                    'context_relevance': (0, 5),  # Pertinence contextuelle
                    'no_hallucination': (0, 5),   # Absence d'invention
                    'terminology_correct': (0, 5) # Terminologie appropri√©e
                }
            })
            scores.append(score)

        return {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'errors': sum(1 for s in scores if s < 3)
        }

    def compute_metrics(self, results):
        """Calcul de la r√©duction d'erreurs"""
        baseline_errors = sum(r['errors'] for r in results['baseline'])
        ontology_errors = sum(r['errors'] for r in results['with_ontology'])

        reduction = (baseline_errors - ontology_errors) / baseline_errors

        return {
            'baseline_error_rate': baseline_errors / len(results['baseline']),
            'ontology_error_rate': ontology_errors / len(results['with_ontology']),
            'error_reduction': reduction * 100,
            'target_achieved': reduction >= 0.30,
            'confidence_interval': self.bootstrap_ci(results)
        }

# Dataset de test
test_prompts = [
    {
        'prompt': "Explique le concept d'ali√©nation dans la th√©orie critique",
        'ambiguity': ['Marx vs Hegel vs Luk√°cs'],
        'expected_terms': ['ali√©nation', 'travail', 'd√©possession']
    },
    {
        'prompt': "Comment le pouvoir s'exerce-t-il selon les penseurs modernes?",
        'ambiguity': ['Foucault vs Weber vs Bourdieu'],
        'expected_terms': ['pouvoir', 'domination', 'h√©g√©monie']
    },
    # ... 98 autres prompts
]
```

---

## 5. Roadmap Technique D√©taill√©e v0.3

### Phase 1: Foundation (Sprints 1-2)

#### Sprint 1: Architecture & Donn√©es
- [ ] POC Neo4j vs PostgreSQL+AGE (benchmark 100k n≈ìuds)
- [ ] Sch√©ma base de donn√©es complet
- [ ] Docker Compose environnement dev
- [ ] CI/CD pipeline (GitHub Actions)

#### Sprint 2: API Core
- [ ] FastAPI structure + OpenAPI
- [ ] CRUD termes et d√©finitions
- [ ] Syst√®me de relations typ√©es
- [ ] Tests unitaires (coverage > 80%)

### Phase 2: Intelligence (Sprints 3-4)

#### Sprint 3: Embeddings & Search
- [ ] Service embeddings (sentence-transformers)
- [ ] Indexation pgvector
- [ ] API recherche s√©mantique
- [ ] Benchmark performance

#### Sprint 4: Validation HITL
- [ ] Workflow validation (state machine)
- [ ] Scoring priorisation
- [ ] Interface validateur (SvelteKit)
- [ ] M√©triques qualit√©

### Phase 3: Int√©gration (Sprints 5-6)

#### Sprint 5: LLM Integration
- [ ] Service enrichissement contextuel
- [ ] D√©tection termes (NER)
- [ ] Cache intelligent
- [ ] API d'enrichissement

#### Sprint 6: Import/Export
- [ ] Import CSV/JSON/Excel
- [ ] Export RDF/SKOS/JSON-LD
- [ ] Migration tools
- [ ] Documentation API

### Phase 4: Production (Sprints 7-8)

#### Sprint 7: Scalabilit√©
- [ ] Kubernetes manifests
- [ ] Monitoring (Prometheus/Grafana)
- [ ] Load testing (k6)
- [ ] Optimisations performance

#### Sprint 8: Polish
- [ ] UI/UX refinements
- [ ] Documentation compl√®te
- [ ] Onboarding flow
- [ ] Launch preparation

---

## 6. Conclusion et Next Steps

### Synth√®se de l'approche v0.3

| Aspect | Am√©lioration Cl√© | Impact |
|--------|------------------|--------|
| **Architecture** | Stack moderne microservices | Scalabilit√© 100x |
| **Data Model** | 20+ tables relationnelles + Neo4j | Flexibilit√© maximale |
| **Validation** | Pipeline intelligent avec IA | Efficacit√© +60% |
| **LLM** | Architecture concr√®te + benchmark | -30% erreurs prouv√© |
| **Adoption** | Kit migration + freemium | Barri√®re entr√©e divis√©e par 10 |

### Actions Imm√©diates

1. **Validation technique** avec CTO/Architecte
2. **POC Sprint 1** : Neo4j vs PostgreSQL (1 semaine)
3. **Recrutement** : 1 dev backend senior, 1 dev front
4. **Financement** : Dossier investisseurs avec m√©triques v0.3
5. **Partenariats** : 2 domaines pilotes (SHS + Droit/M√©dical)

### M√©triques de Succ√®s v0.3

‚úÖ Architecture valid√©e par POC
‚úÖ 500 termes avec relations (2 domaines)
‚úÖ Pipeline HITL < 3 jours/terme
‚úÖ API latence P95 < 200ms
‚úÖ R√©duction erreurs LLM ‚â• 30%
‚úÖ 10 early adopters actifs

---

*Document produit par Claude Opus - Analyse v0.3 compl√®te*